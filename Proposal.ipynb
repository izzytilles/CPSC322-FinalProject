{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Earth Object Classification\n",
    "### By Izzy Tilles and Michael D'Arcy-Evans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "We are using a dataset sourced from Kaggle called [Nearest Earth Objects (1910-2024)](\\\"https://www.kaggle.com/datasets/ivansher/nasa-nearest-earth-objects-1910-2024\\\"). We are planning on looking at `estimated_diameter_min` which is the smallest size scientists think it could be(discretized from 1-10), `estimated_diameter_max` which is the largest size scientists think it could be (discretized from 1-10), `velocity` (discretized from 1-10), and `miss_distance` which is the estimated distance it will be away from Earth when it passes us (discretized from 1-10). We will use those attributes to predict if an object hurtling towards Earth is hazardous, classified as `True` or `False`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation/Technical Merit\n",
    "\n",
    "Anticipated challenges in pre-processing and/or classification:\n",
    "- In order to reasonably interpret this data we will need to discretize every attribute into disjoint bins. On the first pass, discretizing data into bins of uniform width causes a few bins to have zero instances. We will need to develop a strategy to discretize our values more effectively but without making it too specific to our training data. \n",
    "- Our dataset was very large intially, over 330,000 values. Additionally, the class breakdown was ~45,000 `True` cases and ~295,000 `False` cases. The imbalance would have likely caused our model to overclassify instances as `False` since it would have seen a lot more instances that were `False`. Refining the dataset to be a 50-50 split of the 2 classes would alleviate that problem, but we could still have a dataset of 90,000 instances. This would be too large for our classifiers to efficiently go through it. We decided to randomly sample 1,000 instances that were `True` and 1,000 instances that were `False`. By doing this, we aim to have a manageably sized dataset with even amounts of each class so we can create the most accurate classifier as efficiently as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential Impact \n",
    "Our finished project could help space agencies like NASA determine whether to worry about a NEO or not. These findings could assist with policy decisions for government agencies because anticipating a disaster allows for money to be allocated towards relief agencies. We anticipate that our results will be of interest to doomsday preppers as well. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
